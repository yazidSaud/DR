{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  /Users/zeez/opt/anaconda3/bin/python -m pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  /Users/zeez/opt/anaconda3/bin/python -m pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  /Users/zeez/opt/anaconda3/bin/python -m pip install [options] [-e] <vcs project url> ...\n",
      "  /Users/zeez/opt/anaconda3/bin/python -m pip install [options] [-e] <local project path> ...\n",
      "  /Users/zeez/opt/anaconda3/bin/python -m pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: --nltk\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./opt/anaconda3/lib/python3.8/site-packages (3.5)\n",
      "Collecting as\n",
      "  Downloading as-0.1-py3-none-any.whl (2.2 kB)\n",
      "Collecting ntk\n",
      "  Downloading ntk-1.0.2-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: tqdm in ./opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: regex in ./opt/anaconda3/lib/python3.8/site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: click in ./opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in ./opt/anaconda3/lib/python3.8/site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: Pillow in ./opt/anaconda3/lib/python3.8/site-packages (from ntk) (7.2.0)\n",
      "Collecting pyperclip\n",
      "  Downloading pyperclip-1.8.0.tar.gz (16 kB)\n",
      "Building wheels for collected packages: pyperclip\n",
      "  Building wheel for pyperclip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyperclip: filename=pyperclip-1.8.0-py3-none-any.whl size=8691 sha256=2fa83ab1c9353aa653acae620dcea585f7452d95321c10a0470d34631e414deb\n",
      "  Stored in directory: /Users/zeez/Library/Caches/pip/wheels/03/79/58/ab51f0f590281b0f4f6046d9271eb98cc50565afb9200f155a\n",
      "Successfully built pyperclip\n",
      "Installing collected packages: as, pyperclip, ntk\n",
      "Successfully installed as-0.1 ntk-1.0.2 pyperclip-1.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as ntk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/zeez/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Welcome readers. I hope you find it interesting. Please do reply.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome readers.', 'I hope you find it interesting.', 'Please do reply.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome readers.', 'I hope you find it interesting.', 'Please do reply.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['مرحبا بكم.', 'نحن نتعلم اساسيات مبادئ استرجاع المعلومات.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Arabic_text=\"مرحبا بكم. نحن نتعلم اساسيات مبادئ استرجاع المعلومات.\"\n",
    "tokenizer.tokenize(Arabic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'readers',\n",
       " '.',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'find',\n",
       " 'it',\n",
       " 'interesting',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'reply',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=nltk.word_tokenize('Welcome readers. I hope you find it interesting. Please do reply..»')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome', 'readers', '.', 'I', 'hope', 'you', 'find', 'it', 'interesting', '.', 'Please', 'do', 'reply', '..', '»']\n"
     ]
    }
   ],
   "source": [
    "print(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome readers. I hope you find it interesting. Please do reply..»\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'readers',\n",
       " '.',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'find',\n",
       " 'it',\n",
       " 'interesting',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'reply',\n",
       " '..',\n",
       " '»']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texsuu = input()\n",
    "nltk.word_tokenize(texsuu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don',\n",
       " 't',\n",
       " 'hesitate',\n",
       " 'to',\n",
       " 'ask',\n",
       " 'questions',\n",
       " 'or',\n",
       " 'send',\n",
       " 'to',\n",
       " 'me',\n",
       " 'your',\n",
       " 'question',\n",
       " 'to',\n",
       " 'mohsarem',\n",
       " 'gmail',\n",
       " 'com']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=RegexpTokenizer(\"[\\w]+\")\n",
    "tokenizer.tokenize(\"Don't hesitate to ask questions or send to me your question to mohsarem@gmail.com\")\n",
    "                          \n",
    "                          \n",
    "                          \n",
    "                          \n",
    "                          \n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text33=[\" It is a pleasant evening.\",\"Guests, who came from US arrived at the venue\",\"Food was tasty.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'is', 'a', 'pleasant', 'evening', '.'], ['Guests', ',', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Food', 'was', 'tasty', '.']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_docs=[word_tokenize(doc) for doc in text33]\n",
    "print(tokenized_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " IT IS A PLEASANT EVENING.\n"
     ]
    }
   ],
   "source": [
    "print(text33[0].upper())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/zeez/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Don't\", 'hesitate', 'ask', 'questions']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stops=set(stopwords.words('english'))\n",
    "words=[\"Don't\",'hesitate','to','ask','questions']\n",
    "[word for word in words if word not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops=set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"Don't\",'hesitate','to','ask','questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don', 'hesitate', 'ask', 'questions']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stops=set(stopwords.words('english'))\n",
    "txt= ''' Don't hesitate to ask\n",
    "questions'''\n",
    "\n",
    "wordList = re.sub(\"[^\\w]\", \" \",  txt).split()\n",
    "[word for word in wordList if word not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\cocoartf2513\\n\\\\cocoatextscaling0\\\\cocoaplatform0{\\\\fonttbl\\\\f0\\\\fnil\\\\fcharset0 LucidaGrande;}\\n{\\\\colortbl;\\\\red255\\\\green255\\\\blue255;\\\\red169\\\\green14\\\\blue26;\\\\red246\\\\green246\\\\blue246;}\\n{\\\\*\\\\expandedcolortbl;;\\\\cssrgb\\\\c72941\\\\c12941\\\\c12941;\\\\cssrgb\\\\c97255\\\\c97255\\\\c97255;}\\n\\\\paperw11900\\\\paperh16840\\\\margl1440\\\\margr1440\\\\vieww10800\\\\viewh8400\\\\viewkind0\\n\\\\deftab720\\n\\\\pard\\\\pardeftab720\\\\sa240\\\\partightenfactor0\\n\\n\\\\f0\\\\fs29\\\\fsmilli14667 \\\\cf2 \\\\cb3 \\\\expnd0\\\\expndtw0\\\\kerning0\\nYazeed saud one two three}'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "f = open(r'/Users/zeez/Desktop/Untitled.rtf') \n",
    "text = f.read() \n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Have', 'a', 'nice', 'day.', 'You', 'do', 'great', '!']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(\"Have a nice day. You do great!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mohsarem@gmail.com']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=RegexpTokenizer(\"\\S+@\\S+\")\n",
    "tokenizer.tokenize('''Don't hesitate to ask\n",
    "questions or send to me your question to\n",
    "mohsarem@gmail.com''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " IT IS A PLEASANT EVENING.\n",
      " it is a pleasant evening.\n"
     ]
    }
   ],
   "source": [
    "print(text33[0].upper())\n",
    "print(text33[0].lower())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK',\n",
       " 'allows',\n",
       " 'convert',\n",
       " 'Text',\n",
       " 'Lowercase',\n",
       " 'uppercase',\n",
       " 'Don',\n",
       " 'hesitate',\n",
       " 'ask',\n",
       " 'questions']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stops=set(stopwords.words('english'))\n",
    "txt= '''NLTK allows you to convert Text into\n",
    "Lowercase and uppercase. Don't hesitate to ask\n",
    "questions'''\n",
    "\n",
    "wordList = re.sub(\"[^\\w]\", \" \",  txt).split()\n",
    "[word for word in wordList if word not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In computer science, artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. Computer science defines AI research as the study of intelligent agents: any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "f = open(r'/Users/zeez/Desktop/Untitled1.txt')\n",
    "text = f.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'AI',\n",
       " 'sometimes',\n",
       " 'called',\n",
       " 'machine',\n",
       " 'intelligence',\n",
       " 'intelligence',\n",
       " 'demonstrated',\n",
       " 'machines',\n",
       " 'contrast',\n",
       " 'natural',\n",
       " 'intelligence',\n",
       " 'displayed',\n",
       " 'humans',\n",
       " 'animals',\n",
       " 'Computer',\n",
       " 'science',\n",
       " 'defines',\n",
       " 'AI',\n",
       " 'research',\n",
       " 'study',\n",
       " 'intelligent',\n",
       " 'agents',\n",
       " 'device',\n",
       " 'perceives',\n",
       " 'environment',\n",
       " 'takes',\n",
       " 'actions',\n",
       " 'maximize',\n",
       " 'chance',\n",
       " 'successfully',\n",
       " 'achieving',\n",
       " 'goals']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stops=set(stopwords.words('english'))\n",
    "wordList = re.sub(\"[^\\w]\", \" \",  text).split()\n",
    "[word for word in wordList if word not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
